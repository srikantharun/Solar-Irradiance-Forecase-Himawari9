{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solar Power Grid Anomaly Detection using LSTM Autoencoders\n",
    "\n",
    "This notebook demonstrates how to use LSTM autoencoders for anomaly detection in solar power grid sensor data. We'll cover:\n",
    "\n",
    "1. Generating synthetic solar power data with realistic patterns and anomalies\n",
    "2. Implementing and training an LSTM autoencoder model\n",
    "3. Detecting anomalies using reconstruction error\n",
    "4. Evaluating the model's performance\n",
    "5. Visualizing the results\n",
    "\n",
    "The approach is inspired by the paper \"Time Series Anomaly Detection using LSTM Autoencoder\" and adapted for solar power grid applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import custom modules\n",
    "from utils.data_generator import SolarPowerDataGenerator\n",
    "from models.lstm_autoencoder import LSTMAutoencoder, LSTMAutoencoderTrainer\n",
    "from utils.visualization import (\n",
    "    plot_solar_data, plot_daily_patterns, plot_monthly_patterns,\n",
    "    plot_reconstruction_error, plot_feature_reconstruction_error,\n",
    "    plot_error_distribution, plot_tsne_visualization,\n",
    "    plot_anomaly_metrics, plot_confusion_matrix, plot_training_history\n",
    ")\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Solar Power Data\n",
    "\n",
    "We'll start by generating synthetic solar power data that mimics real-world patterns, including:\n",
    "- Daily cycles (sun rises and sets)\n",
    "- Seasonal variations (more power in summer, less in winter)\n",
    "- Weather effects (cloudy days produce less power)\n",
    "- Sensor degradation over time\n",
    "- Random noise and anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Initialize data generator\n",
    "generator = SolarPowerDataGenerator(\n",
    "    n_sensors=5,               # 5 different solar power sensors\n",
    "    start_date=\"2023-01-01\",   # Start from January 1, 2023\n",
    "    end_date=\"2023-06-30\",     # 6 months of data\n",
    "    time_interval=\"15min\",     # 15-minute intervals\n",
    "    anomaly_percentage=0.02,   # 2% of data points will have anomalies\n",
    "    random_seed=42             # For reproducibility\n",
    ")\n",
    "\n",
    "# Generate and save data\n",
    "df_data, df_anomaly = generator.save_data(\n",
    "    \"../data/solar_power_data.csv\",\n",
    "    \"../data/solar_power_anomalies.csv\"\n",
    ")\n",
    "\n",
    "print(f\"Generated dataset with {len(df_data)} timestamps and {df_data.shape[1]} sensors\")\n",
    "print(f\"Time range: {df_data.index[0]} to {df_data.index[-1]}\")\n",
    "print(f\"Total anomalies: {df_anomaly.sum().sum()} ({df_anomaly.sum().sum() / df_anomaly.size * 100:.2f}% of data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the data\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the data\n",
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore and Visualize the Data\n",
    "\n",
    "Let's explore the generated data to understand the patterns and anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one week of data for all sensors with anomalies highlighted\n",
    "fig = plot_solar_data(df_data, days=7, anomalies=df_anomaly)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average daily patterns for all sensors\n",
    "fig = plot_daily_patterns(df_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average monthly patterns for all sensors\n",
    "fig = plot_monthly_patterns(df_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the different types of anomalies in our dataset. Our synthetic data includes:\n",
    "1. **Spikes**: Sudden increases in power output\n",
    "2. **Drops**: Sudden decreases in power output\n",
    "3. **Drifts**: Gradual deviations from expected values\n",
    "4. **Stuck readings**: Sensor values that don't change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find periods with anomalies and plot them\n",
    "anomaly_days = []\n",
    "for sensor in df_data.columns:\n",
    "    # Find dates with anomalies\n",
    "    anomaly_dates = df_anomaly[df_anomaly[sensor] == 1].index.date.unique()\n",
    "    for date in anomaly_dates:\n",
    "        anomaly_days.append(pd.Timestamp(date))\n",
    "\n",
    "# Get unique anomaly days\n",
    "anomaly_days = sorted(set(anomaly_days))\n",
    "\n",
    "# Plot the first 3 anomaly days\n",
    "for i, day in enumerate(anomaly_days[:3]):\n",
    "    # Get start and end of the day\n",
    "    start = pd.Timestamp(day)\n",
    "    end = start + pd.Timedelta(days=1)\n",
    "    \n",
    "    # Filter data for this day\n",
    "    day_data = df_data[(df_data.index >= start) & (df_data.index < end)]\n",
    "    day_anomalies = df_anomaly[(df_anomaly.index >= start) & (df_anomaly.index < end)]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    for sensor in df_data.columns:\n",
    "        plt.plot(day_data.index, day_data[sensor], label=sensor, alpha=0.7)\n",
    "        \n",
    "        # Highlight anomalies\n",
    "        anomaly_idx = day_anomalies.index[day_anomalies[sensor] == 1]\n",
    "        if len(anomaly_idx) > 0:\n",
    "            plt.scatter(anomaly_idx, day_data.loc[anomaly_idx, sensor], \n",
    "                       color='red', marker='x', s=100, \n",
    "                       label=f'{sensor} anomalies' if i == 0 and sensor == df_data.columns[0] else \"\")\n",
    "    \n",
    "    plt.title(f\"Anomalies on {day.strftime('%Y-%m-%d')}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Power (kW)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "sequence_length = 96  # 24 hours (15-min intervals)\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Define model hyperparameters\n",
    "input_dim = df_data.shape[1]  # Number of sensors\n",
    "hidden_dim = 64\n",
    "latent_dim = 32\n",
    "num_layers = 2\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create and Train the LSTM Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = LSTMAutoencoder(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    sequence_length=sequence_length,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = LSTMAutoencoderTrainer(\n",
    "    model=model,\n",
    "    sequence_length=sequence_length,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "history = trainer.train(df_data, epochs=epochs, train_ratio=train_ratio, verbose=True)\n",
    "\n",
    "# Plot training history\n",
    "fig = plot_training_history(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "os.makedirs('../models/saved', exist_ok=True)\n",
    "trainer.save_model('../models/saved/lstm_autoencoder.pt')\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detect Anomalies using Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute reconstruction errors\n",
    "errors, detected_anomalies, thresholds = trainer.detect_anomalies(df_data, threshold_percentile=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reconstruction error over time\n",
    "fig = plot_reconstruction_error(errors, threshold=thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of reconstruction errors\n",
    "fig = plot_error_distribution(errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error by sensor\n",
    "fig = plot_feature_reconstruction_error(errors, sensor_names=df_data.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Anomaly Detection Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "y_true = df_anomaly.values.flatten()  # Ground truth anomalies\n",
    "y_pred = detected_anomalies.flatten()  # Predicted anomalies\n",
    "y_score = errors.flatten()  # Anomaly scores (reconstruction errors)\n",
    "\n",
    "# Calculate precision, recall, F1 score\n",
    "precision = (y_true & y_pred).sum() / y_pred.sum() if y_pred.sum() > 0 else 0\n",
    "recall = (y_true & y_pred).sum() / y_true.sum() if y_true.sum() > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC and Precision-Recall curves\n",
    "fig = plot_anomaly_metrics(y_true, y_score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "fig = plot_confusion_matrix(y_true, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results in Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for detected anomalies\n",
    "df_detected = pd.DataFrame(\n",
    "    detected_anomalies,\n",
    "    index=df_data.index,\n",
    "    columns=df_data.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare true vs detected anomalies for a specific time period\n",
    "start_date = \"2023-03-15\"\n",
    "end_date = \"2023-03-22\"  # One week\n",
    "\n",
    "# Filter data for the specified period\n",
    "period_data = df_data[start_date:end_date]\n",
    "period_true_anomalies = df_anomaly[start_date:end_date]\n",
    "period_detected_anomalies = df_detected[start_date:end_date]\n",
    "\n",
    "# Plot the data for this period with both true and detected anomalies\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "for sensor in df_data.columns:\n",
    "    plt.plot(period_data.index, period_data[sensor], alpha=0.7, label=sensor)\n",
    "    \n",
    "    # Plot true anomalies\n",
    "    true_anomaly_idx = period_true_anomalies.index[period_true_anomalies[sensor] == 1]\n",
    "    if len(true_anomaly_idx) > 0:\n",
    "        plt.scatter(true_anomaly_idx, period_data.loc[true_anomaly_idx, sensor], \n",
    "                   color='green', marker='x', s=100, \n",
    "                   label=f'True anomalies' if sensor == df_data.columns[0] else \"\")\n",
    "    \n",
    "    # Plot detected anomalies\n",
    "    detected_anomaly_idx = period_detected_anomalies.index[period_detected_anomalies[sensor] == 1]\n",
    "    if len(detected_anomaly_idx) > 0:\n",
    "        plt.scatter(detected_anomaly_idx, period_data.loc[detected_anomaly_idx, sensor], \n",
    "                   color='red', marker='o', s=80, facecolors='none',\n",
    "                   label=f'Detected anomalies' if sensor == df_data.columns[0] else \"\")\n",
    "\n",
    "plt.title(f\"Solar Power Data: {start_date} to {end_date}\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Power (kW)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze the Model's Ability to Detect Different Types of Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze specific anomalies\n",
    "def analyze_specific_anomaly(df_data, df_true_anomalies, df_detected_anomalies, date, sensor):\n",
    "    # Get day before and day after for context\n",
    "    start = pd.Timestamp(date) - pd.Timedelta(days=1)\n",
    "    end = pd.Timestamp(date) + pd.Timedelta(days=1)\n",
    "    \n",
    "    # Filter data\n",
    "    period_data = df_data[(df_data.index >= start) & (df_data.index <= end)]\n",
    "    period_true = df_true_anomalies[(df_true_anomalies.index >= start) & (df_true_anomalies.index <= end)]\n",
    "    period_detected = df_detected_anomalies[(df_detected_anomalies.index >= start) & (df_detected_anomalies.index <= end)]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # Plot the data\n",
    "    plt.plot(period_data.index, period_data[sensor], label=sensor, color='blue')\n",
    "    \n",
    "    # Plot true anomalies\n",
    "    true_anomaly_idx = period_true.index[period_true[sensor] == 1]\n",
    "    if len(true_anomaly_idx) > 0:\n",
    "        plt.scatter(true_anomaly_idx, period_data.loc[true_anomaly_idx, sensor], \n",
    "                   color='green', marker='x', s=100, label='True anomalies')\n",
    "    \n",
    "    # Plot detected anomalies\n",
    "    detected_anomaly_idx = period_detected.index[period_detected[sensor] == 1]\n",
    "    if len(detected_anomaly_idx) > 0:\n",
    "        plt.scatter(detected_anomaly_idx, period_data.loc[detected_anomaly_idx, sensor], \n",
    "                   color='red', marker='o', s=80, facecolors='none', label='Detected anomalies')\n",
    "    \n",
    "    plt.title(f\"Anomaly Analysis for {sensor} around {date}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Power (kW)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze\n",
    "    true_count = period_true[sensor].sum()\n",
    "    detected_count = period_detected[sensor].sum()\n",
    "    matched = ((period_true[sensor] == 1) & (period_detected[sensor] == 1)).sum()\n",
    "    \n",
    "    print(f\"Analysis for {sensor} around {date}:\")\n",
    "    print(f\"  - True anomalies: {true_count}\")\n",
    "    print(f\"  - Detected anomalies: {detected_count}\")\n",
    "    print(f\"  - Correctly detected: {matched}\")\n",
    "    print(f\"  - Precision: {matched/detected_count if detected_count > 0 else 0:.2f}\")\n",
    "    print(f\"  - Recall: {matched/true_count if true_count > 0 else 0:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a few interesting anomaly days to analyze\n",
    "interesting_anomalies = []\n",
    "\n",
    "for i, day in enumerate(anomaly_days[:10]):\n",
    "    day_str = day.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Check anomalies for each sensor on this day\n",
    "    for sensor in df_data.columns:\n",
    "        sensor_anomalies = df_anomaly[day_str:day_str][sensor].sum()\n",
    "        if sensor_anomalies > 0:\n",
    "            interesting_anomalies.append((day_str, sensor))\n",
    "            break  # One sensor per day is enough\n",
    "            \n",
    "    if len(interesting_anomalies) >= 3:\n",
    "        break\n",
    "\n",
    "# Analyze each interesting anomaly\n",
    "for date, sensor in interesting_anomalies:\n",
    "    analyze_specific_anomaly(df_data, df_anomaly, df_detected, date, sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Insights\n",
    "\n",
    "Based on our analysis of the LSTM autoencoder for anomaly detection in solar power grid sensor data, we can draw the following conclusions:\n",
    "\n",
    "1. **Model Performance**: The LSTM autoencoder successfully identifies various types of anomalies in solar power data, with a good balance between precision and recall.\n",
    "\n",
    "2. **Types of Anomalies**:\n",
    "   - **Spikes and Drops**: The model is particularly effective at detecting sudden spikes or drops in power output.\n",
    "   - **Drift Anomalies**: Gradual drifts are more challenging to detect but can still be identified with appropriate threshold tuning.\n",
    "   - **Stuck Values**: The model reliably detects when sensors get stuck at a constant value.\n",
    "\n",
    "3. **Threshold Selection**: The choice of threshold percentile significantly impacts the balance between false positives and false negatives. A 99th percentile threshold provides a good balance for this dataset.\n",
    "\n",
    "4. **Real-world Applications**: This approach could be deployed in production solar power grid monitoring systems to:\n",
    "   - Detect sensor malfunctions early\n",
    "   - Identify performance degradation in solar panels\n",
    "   - Alert operators to potential issues in the grid\n",
    "   - Improve overall grid reliability and efficiency\n",
    "\n",
    "5. **Improvements**: Future work could focus on:\n",
    "   - Incorporating weather data to improve the model's context awareness\n",
    "   - Developing more sophisticated thresholding techniques, possibly adaptive thresholds\n",
    "   - Implementing real-time anomaly detection for streaming data\n",
    "   - Creating an explainable AI component to help operators understand why an anomaly was flagged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
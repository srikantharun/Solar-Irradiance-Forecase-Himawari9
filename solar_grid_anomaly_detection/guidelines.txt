Overview

  This repository contains an LSTM-based autoencoder system for detecting anomalies in solar power grid sensor data. It analyzes time-series data from solar sensors to
  identify unusual patterns that may indicate faults, failures, or other issues requiring attention.

  Prerequisites

  1. Python 3.7 or higher
  2. Required packages:
    - PyTorch
    - NumPy
    - Pandas
    - Matplotlib
    - Scikit-learn
    - Seaborn

  Installation Instructions

  1. Clone the repository:
  git clone <repository-url>
  cd solar-grid-anomaly-detection
  2. Set up a Python environment (recommended):
  python -m venv venv
  source venv/bin/activate  # On Windows: venv\Scripts\activate
  3. Install dependencies:
  pip install torch numpy pandas matplotlib scikit-learn seaborn

  Workflow Steps

  1. Data Preparation

  The system works with time-series data from solar grid sensors. Your vendor can use either:

  - Synthetic data: The repository includes a data generator (utils/data_generator.py) to create realistic solar sensor data with anomalies for testing purposes.
  - Real data: To use actual sensor data, ensure it follows this format:
    - Time-indexed DataFrame with datetime as the index
    - Multiple sensor readings as columns (e.g., 'sensor_1', 'sensor_2', etc.)
    - Optionally, include an 'anomaly' column for labeled data if available

  Example using the synthetic data generator:
  from utils.data_generator import generate_solar_sensor_data

  # Generate 30 days of data for 5 sensors with readings every 15 minutes
  data = generate_solar_sensor_data(
      num_days=30,
      num_sensors=5,
      sampling_interval_minutes=15,
      noise_level=0.05,
      anomaly_prob=0.02,  # 2% chance of anomaly at each time point
      anomaly_scale=3.0   # Scale factor for anomalies
  )

  2. Data Preprocessing

  Before training the model:

  1. Create sequences from the time series data:
  from utils.data_generator import create_sequences, train_test_split

  # Define which columns to use as features
  sensor_cols = [f'sensor_{i+1}' for i in range(5)]  # For 5 sensors

  # Create sequences (24 time steps = 6 hours with 15-minute intervals)
  X, y = create_sequences(data, sensor_cols, seq_length=24)

  # Split into training and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

  # Convert to PyTorch tensors
  import torch
  X_train_tensor = torch.FloatTensor(X_train)
  X_test_tensor = torch.FloatTensor(X_test)

  3. Model Training

  Train the LSTM autoencoder model:

  from models.lstm_autoencoder import LSTMAutoencoder
  import torch.optim as optim
  import torch.nn as nn

  # Initialize model
  model = LSTMAutoencoder(
      input_dim=len(sensor_cols),  # Number of sensors
      hidden_dim=64,               # Hidden layer size
      latent_dim=32,               # Latent representation size
      seq_len=24,                  # Sequence length
      num_layers=2                 # Number of LSTM layers
  )

  # Training parameters
  criterion = nn.MSELoss()
  optimizer = optim.Adam(model.parameters(), lr=0.001)
  num_epochs = 50
  batch_size = 32

  # Training loop
  for epoch in range(num_epochs):
      model.train()
      train_loss = 0
      num_batches = len(X_train) // batch_size

      for i in range(num_batches):
          # Get batch
          start_idx = i * batch_size
          end_idx = start_idx + batch_size
          batch_X = X_train_tensor[start_idx:end_idx]

          # Forward pass and loss
          optimizer.zero_grad()
          outputs = model(batch_X)
          loss = criterion(outputs, batch_X)

          # Backward pass
          loss.backward()
          optimizer.step()

          train_loss += loss.item()

      # Print progress
      train_loss /= num_batches
      print(f"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}")

  4. Anomaly Detection

  After training, use the model to detect anomalies:

  # Compute reconstruction error
  model.eval()
  with torch.no_grad():
      # Forward pass
      test_reconstructions = model(X_test_tensor)

      # Get reconstruction error for each sample
      test_errors = model.get_reconstruction_error(X_test_tensor).numpy()

  # Find optimal threshold for anomaly detection
  from utils.visualization import plot_evaluation_metrics
  _, optimal_threshold = plot_evaluation_metrics(y_test, test_errors)

  # Apply threshold to identify anomalies
  anomalies = (test_errors >= optimal_threshold).astype(int)

  5. Visualization and Analysis

  The repository includes visualization utilities to analyze results:

  from utils.visualization import (
      plot_time_series,
      plot_reconstruction,
      plot_reconstruction_error
  )

  # Plot original time series with anomalies
  plot_time_series(data, sensor_cols, anomaly_col='anomaly')

  # Plot original vs reconstructed for a sample
  plot_reconstruction(X_test.numpy(), test_reconstructions.numpy(), idx=0)

  # Plot reconstruction error with threshold
  plot_reconstruction_error(test_errors, threshold=optimal_threshold)

  6. Model Saving and Loading

  Save the trained model for future use:

  # Save model
  torch.save({
      'model_state_dict': model.state_dict(),
      'optimizer_state_dict': optimizer.state_dict(),
      'threshold': optimal_threshold,
      'hyperparameters': {
          'input_dim': len(sensor_cols),
          'hidden_dim': 64,
          'latent_dim': 32,
          'seq_len': 24,
          'num_layers': 2
      }
  }, 'models/lstm_autoencoder.pth')

  Load the model for inference:

  # Load model
  checkpoint = torch.load('models/lstm_autoencoder.pth')
  hyperparameters = checkpoint['hyperparameters']

  # Initialize model with same parameters
  loaded_model = LSTMAutoencoder(
      input_dim=hyperparameters['input_dim'],
      hidden_dim=hyperparameters['hidden_dim'],
      latent_dim=hyperparameters['latent_dim'],
      seq_len=hyperparameters['seq_len'],
      num_layers=hyperparameters['num_layers']
  )

  # Load weights
  loaded_model.load_state_dict(checkpoint['model_state_dict'])
  optimal_threshold = checkpoint['threshold']

  Example Notebook

  For a complete working example, refer to the Jupyter notebook at notebooks/solar_grid_anomaly_detection.ipynb.

  Advanced Usage

  1. Hyperparameter Tuning

  To improve model performance, your vendor might want to tune:
  - hidden_dim and latent_dim: Adjust based on complexity of patterns
  - num_layers: More layers can capture more complex patterns
  - seq_length: Adjust based on the temporal scale of anomalies

  2. Online Learning

  For continuous monitoring:
  1. Start with a pre-trained model
  2. Periodically update with new data
  3. Adjust anomaly threshold as needed

  3. Integration with Alerting Systems

  To build an alerting system:
  1. Process new sensor data in real-time
  2. Apply the trained model to detect anomalies
  3. Trigger alerts when reconstruction error exceeds the threshold

  Troubleshooting

  - If you see high false positives/negatives, adjust the anomaly threshold
  - If model training is unstable, try reducing the learning rate
  - For memory issues with large datasets, reduce batch size or sequence length

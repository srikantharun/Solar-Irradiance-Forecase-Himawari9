{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 90936,
     "databundleVersionId": 10665762,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "### <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n\n#### Maximum Points: 100\n\n<div>\n    <table>\n        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n    </table>\n</div>\n",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n\nIn this section, you have to write a class or methods, which will be used to get training and validation data loader.\n\nYou need to write a custom dataset class to load data.\n\n**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n\n\nFor example:\n\n```python\nclass KenyanFood13Dataset(Dataset):\n    \"\"\"\n    \n    \"\"\"\n    \n    def __init__(self, *args):\n    ....\n    ...\n    \n    def __getitem__(self, idx):\n    ...\n    ...\n    \n\n```\n\n\n```python\ndef get_data(args1, *args):\n    ....\n    ....\n    return train_loader, test_loader\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-20T20:29:09.151199Z",
     "iopub.execute_input": "2025-05-20T20:29:09.151401Z",
     "iopub.status.idle": "2025-05-20T20:29:14.944328Z",
     "shell.execute_reply.started": "2025-05-20T20:29:09.151379Z",
     "shell.execute_reply": "2025-05-20T20:29:14.943426Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "class KenyanFood13Dataset(Dataset):\n    \"\"\"\n    Dataset class for the Kenyan Food 13 dataset.\n    Loads images and corresponding labels from CSV file.\n    \"\"\"\n    \n    def __init__(self, csv_file, img_dir, transform=None, train=True, val_split=0.2, is_test=False, seed=42):\n        \"\"\"\n        Args:\n            csv_file (str): Path to the CSV file with annotations.\n            img_dir (str): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied on a sample.\n            train (bool): If True, create training set, else create validation set.\n            val_split (float): Validation split ratio (default: 0.2).\n            is_test (bool): If True, this is the test set (no labels).\n            seed (int): Random seed for reproducibility.\n        \"\"\"\n        self.img_dir = img_dir\n        self.transform = transform\n        self.is_test = is_test\n        \n        # Read the CSV file\n        self.data_info = pd.read_csv(csv_file)\n        \n        # If this is the test set, we don't need to split or get labels\n        if is_test:\n            self.img_ids = self.data_info['ID'].values\n            self.targets = None\n        else:\n            # Get image IDs and labels\n            self.img_ids = self.data_info['ID'].values\n            \n            # Convert class names to numerical labels\n            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(sorted(self.data_info['CLASS'].unique()))}\n            self.idx_to_class = {i: cls_name for cls_name, i in self.class_to_idx.items()}\n            self.targets = [self.class_to_idx[cls] for cls in self.data_info['CLASS'].values]\n            \n            # Split into train and validation sets\n            train_indices, val_indices = train_test_split(\n                np.arange(len(self.img_ids)),\n                test_size=val_split,\n                random_state=seed,\n                stratify=self.targets\n            )\n            \n            # Select either training or validation indices\n            if train:\n                self.indices = train_indices\n            else:\n                self.indices = val_indices\n                \n            # Filter image IDs and targets\n            self.img_ids = self.img_ids[self.indices]\n            self.targets = [self.targets[i] for i in self.indices]\n    \n    def __len__(self):\n        \"\"\"Return the total number of samples\"\"\"\n        return len(self.img_ids)\n    \n    def __getitem__(self, idx):\n        \"\"\"Get a sample from the dataset\"\"\"\n        img_id = self.img_ids[idx]\n        img_path = os.path.join(self.img_dir, f\"{img_id}.jpg\")\n        \n        # Load the image\n        try:\n            image = Image.open(img_path).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image {img_path}: {e}\")\n            # Return a black image and its label if the image can't be loaded\n            image = Image.new('RGB', (224, 224))\n        \n        # Apply transformations\n        if self.transform:\n            image = self.transform(image)\n        \n        # Return image and label for training/val, just image for test\n        if self.is_test:\n            return image, img_id\n        else:\n            return image, self.targets[idx]",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-20T20:29:29.447108Z",
     "iopub.execute_input": "2025-05-20T20:29:29.447422Z",
     "iopub.status.idle": "2025-05-20T20:29:29.456420Z",
     "shell.execute_reply.started": "2025-05-20T20:29:29.447399Z",
     "shell.execute_reply": "2025-05-20T20:29:29.455567Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "def get_data(data_dir, batch_size=32, num_workers=4):\n    \"\"\"\n    Create train, validation and test data loaders.\n    \n    Args:\n        data_dir (str): Path to the data directory.\n        batch_size (int): Batch size for the data loaders.\n        num_workers (int): Number of workers for the data loaders.\n        \n    Returns:\n        train_loader, val_loader, test_loader: DataLoader objects for training, validation and testing.\n    \"\"\"\n    # Define paths\n    train_csv = os.path.join(data_dir, 'train.csv')\n    test_csv = os.path.join(data_dir, 'test.csv')\n    img_dir = os.path.join(data_dir, 'images')\n    \n    # Define transformations\n    train_transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.RandomCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    val_test_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create datasets\n    train_dataset = KenyanFood13Dataset(train_csv, img_dir, transform=train_transform, train=True)\n    val_dataset = KenyanFood13Dataset(train_csv, img_dir, transform=val_test_transform, train=False)\n    test_dataset = KenyanFood13Dataset(test_csv, img_dir, transform=val_test_transform, is_test=True)\n    \n    # Get class weights for handling imbalanced classes\n    class_counts = np.bincount(train_dataset.targets)\n    class_weights = 1.0 / class_counts\n    weights = class_weights[train_dataset.targets]\n    sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, sampler=sampler,\n        num_workers=num_workers, pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=True\n    )\n    \n    return train_loader, val_loader, test_loader",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-20T20:29:34.293762Z",
     "iopub.execute_input": "2025-05-20T20:29:34.294081Z",
     "iopub.status.idle": "2025-05-20T20:29:34.301156Z",
     "shell.execute_reply.started": "2025-05-20T20:29:34.294061Z",
     "shell.execute_reply": "2025-05-20T20:29:34.300398Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "## <font style=\"color:green\">2. Configuration [5 Points]</font>\n\n**Define your configuration here.**\n\nFor example:\n\n\n```python\n@dataclass\nclass TrainingConfiguration:\n    '''\n    Describes configuration of the training process\n    '''\n    batch_size: int = 10 \n    epochs_count: int = 50  \n    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n    log_interval: int = 5  \n    test_interval: int = 1  \n    data_root: str = \"/kaggle/input/opencv-pytorch-project-2-classification-round-3\" \n    num_workers: int = 2  \n    device: str = 'cuda'  \n    \n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from dataclasses import dataclass, field\nimport torch\nimport os\nfrom typing import List, Optional, Tuple\n\n@dataclass\nclass TrainingConfiguration:\n    \"\"\"Configuration for training the Kenyan Food 13 classifier.\"\"\"\n    \n    # Data parameters\n    data_dir: str = \"/kaggle/input/opencv-pytorch-project-2-classification-round-3\"\n    img_dir: str = \"images\"\n    train_csv: str = \"train.csv\"\n    test_csv: str = \"test.csv\"\n    val_split: float = 0.2\n    num_classes: int = 13\n    \n    # Image parameters\n    img_size: int = 224\n    crop_size: int = 224\n    resize_size: int = 256\n    mean: Tuple[float, float, float] = (0.485, 0.456, 0.406)  # ImageNet means\n    std: Tuple[float, float, float] = (0.229, 0.224, 0.225)   # ImageNet stds\n    \n    # Augmentation parameters\n    use_augmentation: bool = True\n    rotation_degrees: int = 15\n    color_jitter_factor: float = 0.1\n    \n    # Model parameters\n    model_name: str = \"resnet50\"  # Options: resnet18, resnet50, efficientnet_b0, etc.\n    pretrained: bool = True\n    dropout_rate: float = 0.2\n    \n    # Training parameters\n    batch_size: int = 32\n    num_epochs: int = 30\n    learning_rate: float = 1e-3\n    weight_decay: float = 1e-4\n    lr_scheduler: str = \"cosine\"  # Options: cosine, step, plateau\n    lr_step_size: int = 7\n    lr_gamma: float = 0.1\n    lr_min: float = 1e-6\n    early_stopping_patience: int = 5\n    \n    # Optimizer parameters\n    optimizer: str = \"adamw\"  # Options: adam, adamw, sgd\n    momentum: float = 0.9  # For SGD\n    \n    # Loss parameters\n    loss_fn: str = \"cross_entropy\"  # Options: cross_entropy, focal\n    focal_alpha: float = 0.25\n    focal_gamma: float = 2.0\n    \n    # Device parameters\n    device: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n    num_workers: int = 4\n    pin_memory: bool = True\n    \n    # Logging parameters\n    checkpoint_dir: str = \"./checkpoints\"\n    tensorboard_dir: str = \"./runs\"\n    log_interval: int = 10  # Log every N batches\n    save_best_only: bool = True\n    \n    # Miscellaneous\n    seed: int = 42\n    verbose: bool = True\n    mixed_precision: bool = True  # Use mixed precision training\n    \n    def __post_init__(self):\n        \"\"\"Create directories if they don't exist.\"\"\"\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n        os.makedirs(self.tensorboard_dir, exist_ok=True)\n    \n    def get_full_img_dir(self) -> str:\n        \"\"\"Get the full path to the image directory.\"\"\"\n        return os.path.join(self.data_dir, self.img_dir)\n    \n    def get_full_train_csv(self) -> str:\n        \"\"\"Get the full path to the training CSV file.\"\"\"\n        return os.path.join(self.data_dir, self.train_csv)\n    \n    def get_full_test_csv(self) -> str:\n        \"\"\"Get the full path to the test CSV file.\"\"\"\n        return os.path.join(self.data_dir, self.test_csv)\n    \n    def model_checkpoint_path(self, epoch: Optional[int] = None) -> str:\n        \"\"\"Get the path to save/load model checkpoints.\"\"\"\n        if epoch is not None:\n            return os.path.join(self.checkpoint_dir, f\"{self.model_name}_epoch_{epoch}.pth\")\n        return os.path.join(self.checkpoint_dir, f\"{self.model_name}_best.pth\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-20T20:32:04.592143Z",
     "iopub.execute_input": "2025-05-20T20:32:04.592559Z",
     "iopub.status.idle": "2025-05-20T20:32:04.603381Z",
     "shell.execute_reply.started": "2025-05-20T20:32:04.592529Z",
     "shell.execute_reply": "2025-05-20T20:32:04.602547Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n\n**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.metrics import classification_report\nimport pandas as pd\nfrom typing import Dict, List, Tuple, Optional, Union\nimport torch.nn.functional as F\n\nclass EvaluationMetrics:\n    \"\"\"\n    Class for evaluating model performance on the Kenyan Food classification task.\n    Includes methods for computing accuracy, precision, recall, F1-score, and \n    generating confusion matrices and other visualizations.\n    \"\"\"\n    \n    @staticmethod\n    def accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n        \"\"\"\n        Calculate classification accuracy.\n        \n        Args:\n            y_true: Ground truth labels\n            y_pred: Predicted labels\n            \n        Returns:\n            Accuracy score\n        \"\"\"\n        return accuracy_score(y_true, y_pred)\n    \n    @staticmethod\n    def precision(y_true: np.ndarray, y_pred: np.ndarray, average: str = 'weighted') -> float:\n        \"\"\"\n        Calculate precision score.\n        \n        Args:\n            y_true: Ground truth labels\n            y_pred: Predicted labels\n            average: Averaging method ('micro', 'macro', 'weighted', or None for per-class)\n            \n        Returns:\n            Precision score\n        \"\"\"\n        return precision_score(y_true, y_pred, average=average, zero_division=0)\n    \n    @staticmethod\n    def recall(y_true: np.ndarray, y_pred: np.ndarray, average: str = 'weighted') -> float:\n        \"\"\"\n        Calculate recall score.\n        \n        Args:\n            y_true: Ground truth labels\n            y_pred: Predicted labels\n            average: Averaging method ('micro', 'macro', 'weighted', or None for per-class)\n            \n        Returns:\n            Recall score\n        \"\"\"\n        return recall_score(y_true, y_pred, average=average, zero_division=0)\n    \n    @staticmethod\n    def f1(y_true: np.ndarray, y_pred: np.ndarray, average: str = 'weighted') -> float:\n        \"\"\"\n        Calculate F1 score.\n        \n        Args:\n            y_true: Ground truth labels\n            y_pred: Predicted labels\n            average: Averaging method ('micro', 'macro', 'weighted', or None for per-class)\n            \n        Returns:\n            F1 score\n        \"\"\"\n        return f1_score(y_true, y_pred, average=average, zero_division=0)\n    \n    @staticmethod\n    def confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Generate confusion matrix.\n        \n        Args:\n            y_true: Ground truth labels\n            y_pred: Predicted labels\n            \n        Returns:\n            Confusion matrix as a numpy array\n        \"\"\"\n        return confusion_matrix(y_true, y_pred)\n    \n    @staticmethod\n    def plot_confusion_matrix(\n        y_true: np.ndarray, \n        y_pred: np.ndarray, \n        class_names: List[str],\n        figsize: Tuple[int, int] = (12, 10),\n        normalize: bool = True\n    ) -> plt.Figure:\n        \"\"\"\n        Plot confusion matrix with class names.\n        \n        Args:\n            y_true: Ground truth labels\n            y_pred: Predicted labels\n            class_names: List of class names\n            figsize: Figure size\n            normalize: Whether to normalize the confusion matrix\n            \n        Returns:\n            Matplotlib figure object\n        \"\"\"\n        cm = confusion_matrix(y_true, y_pred)\n        \n        if normalize:\n            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n            \n        fig, ax = plt.subplots(figsize=figsize)\n        sns.heatmap(\n            cm, annot=True, fmt='.2f' if normalize else 'd',\n            cmap='Blues', xticklabels=class_names, yticklabels=class_names\n        )\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.title('Confusion Matrix')\n        plt.tight_layout()\n        \n        return fig\n    \n    @staticmethod\n    def plot_metrics_history(metrics_history: Dict[str, List[float]]) -> plt.Figure:\n        \"\"\"\n        Plot the history of metrics during training.\n        \n        Args:\n            metrics_history: Dictionary of metric name to list of values\n            \n        Returns:\n            Matplotlib figure object\n        \"\"\"\n        fig, ax = plt.subplots(figsize=(12, 6))\n        \n        for metric_name, values in metrics_history.items():\n            ax.plot(values, label=metric_name)\n            \n        ax.set_xlabel('Epoch')\n        ax.set_ylabel('Value')\n        ax.set_title('Training Metrics History')\n        ax.legend()\n        ax.grid(True)\n        \n        return fig\n    \n    @staticmethod\n    def classification_report_df(\n        y_true: np.ndarray, \n        y_pred: np.ndarray, \n        class_names: List[str]\n    ) -> pd.DataFrame:\n        \"\"\"\n        Generate a classification report as a pandas DataFrame.\n        \n        Args:\n            y_true: Ground truth labels\n            y_pred: Predicted labels\n            class_names: List of class names\n            \n        Returns:\n            DataFrame with precision, recall, and f1-score for each class\n        \"\"\"\n        report = classification_report(\n            y_true, y_pred, target_names=class_names, output_dict=True\n        )\n        return pd.DataFrame(report).transpose()\n    \n    @staticmethod\n    def top_k_accuracy(\n        outputs: torch.Tensor, \n        targets: torch.Tensor, \n        k: int = 5\n    ) -> float:\n        \"\"\"\n        Calculate top-k accuracy.\n        \n        Args:\n            outputs: Model outputs (logits) of shape [batch_size, num_classes]\n            targets: Ground truth labels of shape [batch_size]\n            k: k value for top-k accuracy\n            \n        Returns:\n            Top-k accuracy\n        \"\"\"\n        batch_size = targets.size(0)\n        _, pred = outputs.topk(k, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(targets.view(1, -1).expand_as(pred))\n        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n        return correct_k.item() * (100.0 / batch_size)\n    \n    @staticmethod\n    def per_class_accuracy(\n        y_true: np.ndarray, \n        y_pred: np.ndarray, \n        num_classes: int\n    ) -> np.ndarray:\n        \"\"\"\n        Calculate per-class accuracy.\n        \n        Args:\n            y_true: Ground truth labels\n            y_pred: Predicted labels\n            num_classes: Number of classes\n            \n        Returns:\n            Array of per-class accuracy values\n        \"\"\"\n        per_class_acc = np.zeros(num_classes)\n        \n        for i in range(num_classes):\n            idx = y_true == i\n            if np.sum(idx) > 0:\n                per_class_acc[i] = accuracy_score(y_true[idx], y_pred[idx])\n                \n        return per_class_acc\n    \n    @staticmethod\n    def plot_per_class_metrics(\n        y_true: np.ndarray, \n        y_pred: np.ndarray, \n        class_names: List[str]\n    ) -> plt.Figure:\n        \"\"\"\n        Plot per-class precision, recall, and F1 score.\n        \n        Args:\n            y_true: Ground truth labels\n            y_pred: Predicted labels\n            class_names: List of class names\n            \n        Returns:\n            Matplotlib figure object\n        \"\"\"\n        precision = precision_score(y_true, y_pred, average=None, zero_division=0)\n        recall = recall_score(y_true, y_pred, average=None, zero_division=0)\n        f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n        \n        fig, ax = plt.subplots(figsize=(15, 8))\n        x = np.arange(len(class_names))\n        width = 0.2\n        \n        ax.bar(x - width, precision, width, label='Precision')\n        ax.bar(x, recall, width, label='Recall')\n        ax.bar(x + width, f1, width, label='F1-score')\n        \n        ax.set_ylabel('Score')\n        ax.set_title('Per-class Performance Metrics')\n        ax.set_xticks(x)\n        ax.set_xticklabels(class_names, rotation=45, ha='right')\n        ax.legend()\n        plt.tight_layout()\n        \n        return fig\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-20T20:34:06.522062Z",
     "iopub.execute_input": "2025-05-20T20:34:06.522437Z",
     "iopub.status.idle": "2025-05-20T20:34:06.849696Z",
     "shell.execute_reply.started": "2025-05-20T20:34:06.522411Z",
     "shell.execute_reply": "2025-05-20T20:34:06.849044Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "class MetricsTracker:\n    \"\"\"\n    Class for tracking and logging metrics during training.\n    \"\"\"\n    \n    def __init__(self, num_classes: int, class_names: Optional[List[str]] = None):\n        \"\"\"\n        Initialize metrics tracker.\n        \n        Args:\n            num_classes: Number of classes\n            class_names: List of class names (optional)\n        \"\"\"\n        self.num_classes = num_classes\n        self.class_names = class_names if class_names else [f\"Class {i}\" for i in range(num_classes)]\n        self.reset()\n        \n    def reset(self):\n        \"\"\"Reset metrics for a new epoch.\"\"\"\n        self.true_labels = []\n        self.pred_labels = []\n        self.logits = []\n        self.loss_values = []\n        self.metrics_history = {\n            'train_loss': [],\n            'val_loss': [],\n            'train_acc': [],\n            'val_acc': [],\n            'train_f1': [],\n            'val_f1': []\n        }\n        \n    def update(\n        self, \n        outputs: torch.Tensor, \n        targets: torch.Tensor, \n        loss: Optional[torch.Tensor] = None\n    ):\n        \"\"\"\n        Update metrics with batch results.\n        \n        Args:\n            outputs: Model outputs (logits)\n            targets: Ground truth labels\n            loss: Loss value (optional)\n        \"\"\"\n        # Convert to numpy for metric calculation\n        _, preds = torch.max(outputs, 1)\n        self.true_labels.extend(targets.cpu().numpy())\n        self.pred_labels.extend(preds.cpu().numpy())\n        self.logits.append(outputs.detach().cpu())\n        \n        if loss is not None:\n            self.loss_values.append(loss.item())\n            \n    def compute_metrics(self) -> Dict[str, float]:\n        \"\"\"\n        Compute metrics from accumulated batch results.\n        \n        Returns:\n            Dictionary of metric names and values\n        \"\"\"\n        y_true = np.array(self.true_labels)\n        y_pred = np.array(self.pred_labels)\n        \n        metrics = {\n            'accuracy': EvaluationMetrics.accuracy(y_true, y_pred),\n            'precision': EvaluationMetrics.precision(y_true, y_pred),\n            'recall': EvaluationMetrics.recall(y_true, y_pred),\n            'f1_score': EvaluationMetrics.f1(y_true, y_pred),\n            'loss': np.mean(self.loss_values) if self.loss_values else None\n        }\n        \n        return metrics\n    \n    def update_history(self, phase: str, metrics: Dict[str, float]):\n        \"\"\"\n        Update metrics history with current epoch results.\n        \n        Args:\n            phase: 'train' or 'val'\n            metrics: Dictionary of metric names and values\n        \"\"\"\n        self.metrics_history[f'{phase}_loss'].append(metrics['loss'])\n        self.metrics_history[f'{phase}_acc'].append(metrics['accuracy'])\n        self.metrics_history[f'{phase}_f1'].append(metrics['f1_score'])\n        \n    def log_metrics(self, epoch: int, phase: str, metrics: Dict[str, float]):\n        \"\"\"\n        Log metrics for current epoch.\n        \n        Args:\n            epoch: Current epoch number\n            phase: 'train' or 'val'\n            metrics: Dictionary of metric names and values\n        \"\"\"\n        log_str = f\"Epoch {epoch} - {phase.capitalize()} - \"\n        log_str += \" | \".join([f\"{k}: {v:.4f}\" for k, v in metrics.items() if v is not None])\n        print(log_str)\n        \n    def generate_report(self) -> pd.DataFrame:\n        \"\"\"\n        Generate a classification report.\n        \n        Returns:\n            DataFrame with precision, recall, and f1-score for each class\n        \"\"\"\n        y_true = np.array(self.true_labels)\n        y_pred = np.array(self.pred_labels)\n        \n        return EvaluationMetrics.classification_report_df(y_true, y_pred, self.class_names)\n    \n    def plot_confusion_matrix(self, normalize: bool = True) -> plt.Figure:\n        \"\"\"\n        Plot confusion matrix.\n        \n        Args:\n            normalize: Whether to normalize the confusion matrix\n            \n        Returns:\n            Matplotlib figure object\n        \"\"\"\n        y_true = np.array(self.true_labels)\n        y_pred = np.array(self.pred_labels)\n        \n        return EvaluationMetrics.plot_confusion_matrix(\n            y_true, y_pred, self.class_names, normalize=normalize\n        )\n    \n    def plot_metrics_history(self) -> plt.Figure:\n        \"\"\"\n        Plot metrics history.\n        \n        Returns:\n            Matplotlib figure object\n        \"\"\"\n        return EvaluationMetrics.plot_metrics_history(self.metrics_history)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-20T20:34:12.842967Z",
     "iopub.execute_input": "2025-05-20T20:34:12.843394Z",
     "iopub.status.idle": "2025-05-20T20:34:12.855222Z",
     "shell.execute_reply.started": "2025-05-20T20:34:12.843370Z",
     "shell.execute_reply": "2025-05-20T20:34:12.854273Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "def get_predictions(model, dataloader, device):\n    \"\"\"\n    Get model predictions on a dataset.\n    \n    Args:\n        model: PyTorch model\n        dataloader: DataLoader for the dataset\n        device: Device to run inference on\n        \n    Returns:\n        Tuple of (true labels, predicted labels, raw outputs)\n    \"\"\"\n    model.eval()\n    all_outputs = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(inputs)\n            all_outputs.append(outputs)\n            all_labels.append(labels)\n    \n    all_outputs = torch.cat(all_outputs, dim=0)\n    all_labels = torch.cat(all_labels, dim=0)\n    _, predicted = torch.max(all_outputs, 1)\n    \n    return all_labels.cpu().numpy(), predicted.cpu().numpy(), all_outputs.cpu()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-20T20:34:17.125976Z",
     "iopub.execute_input": "2025-05-20T20:34:17.126253Z",
     "iopub.status.idle": "2025-05-20T20:34:17.131769Z",
     "shell.execute_reply.started": "2025-05-20T20:34:17.126233Z",
     "shell.execute_reply": "2025-05-20T20:34:17.130894Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": "## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n\n\n**Write the methods or classes to be used for training and validation.**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, StepLR\nimport time\nimport os\nimport copy\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple, Union, Callable\n\n\nclass Trainer:\n    \"\"\"\n    Class for training and validating models for the Kenyan Food 13 classification task.\n    \"\"\"\n    \n    def __init__(\n        self,\n        model: nn.Module,\n        config: 'TrainingConfiguration',\n        train_loader: torch.utils.data.DataLoader,\n        val_loader: torch.utils.data.DataLoader,\n        metrics_tracker: 'MetricsTracker',\n        criterion: Optional[nn.Module] = None,\n        optimizer: Optional[torch.optim.Optimizer] = None,\n        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n        device: Optional[torch.device] = None\n    ):\n        \"\"\"\n        Initialize the trainer.\n        \n        Args:\n            model: PyTorch model to train\n            config: Training configuration\n            train_loader: DataLoader for training data\n            val_loader: DataLoader for validation data\n            metrics_tracker: Metrics tracker for logging\n            criterion: Loss function (if None, CrossEntropyLoss is used)\n            optimizer: Optimizer (if None, AdamW is used)\n            scheduler: Learning rate scheduler (if None, created based on config)\n            device: Device to train on (if None, use config.device)\n        \"\"\"\n        self.model = model\n        self.config = config\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.metrics_tracker = metrics_tracker\n        self.device = device if device is not None else torch.device(config.device)\n        self.model = self.model.to(self.device)\n        \n        # Set up criterion\n        self.criterion = criterion if criterion is not None else nn.CrossEntropyLoss()\n        \n        # Set up optimizer\n        if optimizer is not None:\n            self.optimizer = optimizer\n        else:\n            if config.optimizer.lower() == 'adam':\n                self.optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n            elif config.optimizer.lower() == 'adamw':\n                self.optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n            elif config.optimizer.lower() == 'sgd':\n                self.optimizer = optim.SGD(\n                    model.parameters(), \n                    lr=config.learning_rate,\n                    momentum=config.momentum,\n                    weight_decay=config.weight_decay\n                )\n            else:\n                raise ValueError(f\"Unsupported optimizer: {config.optimizer}\")\n        \n        # Set up scheduler\n        if scheduler is not None:\n            self.scheduler = scheduler\n        else:\n            if config.lr_scheduler.lower() == 'plateau':\n                self.scheduler = ReduceLROnPlateau(\n                    self.optimizer, mode='max', factor=config.lr_gamma, \n                    patience=3, verbose=True\n                )\n            elif config.lr_scheduler.lower() == 'cosine':\n                self.scheduler = CosineAnnealingLR(\n                    self.optimizer, T_max=config.num_epochs, eta_min=config.lr_min\n                )\n            elif config.lr_scheduler.lower() == 'step':\n                self.scheduler = StepLR(\n                    self.optimizer, step_size=config.lr_step_size, gamma=config.lr_gamma\n                )\n            else:\n                self.scheduler = None\n        \n        # Set up tensorboard writer\n        self.writer = SummaryWriter(config.tensorboard_dir)\n        \n        # Initialize best model metrics\n        self.best_val_metric = 0.0\n        self.best_model_wts = copy.deepcopy(model.state_dict())\n        self.early_stopping_counter = 0\n        \n    def train_one_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Train for one epoch.\n        \n        Args:\n            epoch: Current epoch number\n            \n        Returns:\n            Dictionary of metric values for the epoch\n        \"\"\"\n        self.model.train()\n        self.metrics_tracker.reset()\n        running_loss = 0.0\n        \n        # Use tqdm for progress bar\n        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{self.config.num_epochs} [Train]\")\n        \n        # Enable mixed precision if configured\n        scaler = torch.cuda.amp.GradScaler() if self.config.mixed_precision else None\n        \n        for i, (inputs, targets) in enumerate(pbar):\n            inputs = inputs.to(self.device)\n            targets = targets.to(self.device)\n            \n            # Zero the parameter gradients\n            self.optimizer.zero_grad()\n            \n            # Forward\n            with torch.set_grad_enabled(True):\n                with torch.cuda.amp.autocast() if self.config.mixed_precision else torch.no_grad():\n                    outputs = self.model(inputs)\n                    loss = self.criterion(outputs, targets)\n                \n                # Backward + optimize\n                if scaler is not None:\n                    scaler.scale(loss).backward()\n                    scaler.step(self.optimizer)\n                    scaler.update()\n                else:\n                    loss.backward()\n                    self.optimizer.step()\n            \n            # Update metrics\n            self.metrics_tracker.update(outputs, targets, loss)\n            running_loss += loss.item() * inputs.size(0)\n            \n            # Update progress bar\n            if i % self.config.log_interval == 0:\n                pbar.set_postfix({\n                    'loss': loss.item(),\n                    'lr': self.optimizer.param_groups[0]['lr']\n                })\n        \n        # Compute metrics for the entire epoch\n        metrics = self.metrics_tracker.compute_metrics()\n        \n        # Log metrics to tensorboard\n        self.writer.add_scalar('Loss/train', metrics['loss'], epoch)\n        self.writer.add_scalar('Accuracy/train', metrics['accuracy'], epoch)\n        self.writer.add_scalar('F1/train', metrics['f1_score'], epoch)\n        \n        # Update metrics history\n        self.metrics_tracker.update_history('train', metrics)\n        \n        # Log metrics\n        self.metrics_tracker.log_metrics(epoch + 1, 'train', metrics)\n        \n        return metrics\n    \n    def validate(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Validate the model.\n        \n        Args:\n            epoch: Current epoch number\n            \n        Returns:\n            Dictionary of metric values for validation\n        \"\"\"\n        self.model.eval()\n        self.metrics_tracker.reset()\n        running_loss = 0.0\n        \n        # Use tqdm for progress bar\n        pbar = tqdm(self.val_loader, desc=f\"Epoch {epoch+1}/{self.config.num_epochs} [Val]\")\n        \n        with torch.no_grad():\n            for inputs, targets in pbar:\n                inputs = inputs.to(self.device)\n                targets = targets.to(self.device)\n                \n                # Forward\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, targets)\n                \n                # Update metrics\n                self.metrics_tracker.update(outputs, targets, loss)\n                running_loss += loss.item() * inputs.size(0)\n        \n        # Compute metrics for validation\n        metrics = self.metrics_tracker.compute_metrics()\n        \n        # Log metrics to tensorboard\n        self.writer.add_scalar('Loss/val', metrics['loss'], epoch)\n        self.writer.add_scalar('Accuracy/val', metrics['accuracy'], epoch)\n        self.writer.add_scalar('F1/val', metrics['f1_score'], epoch)\n        \n        # Update metrics history\n        self.metrics_tracker.update_history('val', metrics)\n        \n        # Log metrics\n        self.metrics_tracker.log_metrics(epoch + 1, 'val', metrics)\n        \n        # Update learning rate scheduler if using ReduceLROnPlateau\n        if isinstance(self.scheduler, ReduceLROnPlateau):\n            self.scheduler.step(metrics['accuracy'])\n        \n        return metrics\n    \n    def train(self, monitor_metric: str = 'accuracy') -> nn.Module:\n        \"\"\"\n        Train the model for the specified number of epochs.\n        \n        Args:\n            monitor_metric: Metric to monitor for early stopping and best model ('accuracy', 'f1_score')\n            \n        Returns:\n            Best model based on validation metric\n        \"\"\"\n        print(f\"Starting training for {self.config.num_epochs} epochs...\")\n        start_time = time.time()\n        \n        # Train for each epoch\n        for epoch in range(self.config.num_epochs):\n            # Train for one epoch\n            train_metrics = self.train_one_epoch(epoch)\n            \n            # Validate\n            val_metrics = self.validate(epoch)\n            \n            # Update scheduler if not ReduceLROnPlateau\n            if self.scheduler is not None and not isinstance(self.scheduler, ReduceLROnPlateau):\n                self.scheduler.step()\n            \n            # Check if this is the best model\n            current_val_metric = val_metrics[monitor_metric]\n            if current_val_metric > self.best_val_metric:\n                self.best_val_metric = current_val_metric\n                self.best_model_wts = copy.deepcopy(self.model.state_dict())\n                self.early_stopping_counter = 0\n                \n                # Save the best model\n                if self.config.save_best_only:\n                    self.save_checkpoint(epoch, monitor_metric=monitor_metric, is_best=True)\n                    print(f\"Saved best model with {monitor_metric}: {current_val_metric:.4f}\")\n            else:\n                self.early_stopping_counter += 1\n                if self.early_stopping_counter >= self.config.early_stopping_patience:\n                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n                    break\n            \n            # Save checkpoint if not saving only the best model\n            if not self.config.save_best_only:\n                self.save_checkpoint(epoch, monitor_metric=monitor_metric)\n        \n        # Load the best model\n        self.model.load_state_dict(self.best_model_wts)\n        \n        # Calculate elapsed time\n        time_elapsed = time.time() - start_time\n        print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n        print(f\"Best validation {monitor_metric}: {self.best_val_metric:.4f}\")\n        \n        # Close tensorboard writer\n        self.writer.close()\n        \n        return self.model\n    \n    def save_checkpoint(self, epoch: int, monitor_metric: str = 'accuracy', is_best: bool = False) -> str:\n        \"\"\"\n        Save model checkpoint.\n        \n        Args:\n            epoch: Current epoch number\n            monitor_metric: Metric being monitored\n            is_best: Whether this is the best model so far\n            \n        Returns:\n            Path to saved checkpoint\n        \"\"\"\n        if is_best:\n            checkpoint_path = self.config.model_checkpoint_path()\n        else:\n            checkpoint_path = self.config.model_checkpoint_path(epoch=epoch)\n        \n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n            f'best_{monitor_metric}': self.best_val_metric,\n            'config': self.config\n        }\n        \n        torch.save(checkpoint, checkpoint_path)\n        return checkpoint_path\n    \n    def load_checkpoint(self, checkpoint_path: str) -> int:\n        \"\"\"\n        Load model checkpoint.\n        \n        Args:\n            checkpoint_path: Path to checkpoint\n            \n        Returns:\n            Epoch number of the checkpoint\n        \"\"\"\n        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        if self.scheduler and checkpoint['scheduler_state_dict']:\n            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        \n        return checkpoint['epoch']",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-20T20:35:09.939844Z",
     "iopub.execute_input": "2025-05-20T20:35:09.940177Z",
     "iopub.status.idle": "2025-05-20T20:35:17.729587Z",
     "shell.execute_reply.started": "2025-05-20T20:35:09.940149Z",
     "shell.execute_reply": "2025-05-20T20:35:17.728517Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "def train_and_validate(\n    model: nn.Module, \n    train_loader: torch.utils.data.DataLoader,\n    val_loader: torch.utils.data.DataLoader,\n    config: 'TrainingConfiguration',\n    class_names: List[str]\n) -> Tuple[nn.Module, Dict[str, List[float]], float]:\n    \"\"\"\n    Train and validate a model.\n    \n    Args:\n        model: PyTorch model to train\n        train_loader: DataLoader for training data\n        val_loader: DataLoader for validation data\n        config: Training configuration\n        class_names: List of class names\n        \n    Returns:\n        Tuple of (best model, metrics history, best validation accuracy)\n    \"\"\"\n    # Set random seeds for reproducibility\n    torch.manual_seed(config.seed)\n    np.random.seed(config.seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(config.seed)\n    \n    # Initialize metrics tracker\n    metrics_tracker = MetricsTracker(num_classes=config.num_classes, class_names=class_names)\n    \n    # Initialize trainer\n    trainer = Trainer(\n        model=model,\n        config=config,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        metrics_tracker=metrics_tracker\n    )\n    \n    # Train the model\n    best_model = trainer.train(monitor_metric='accuracy')\n    \n    # Get metrics history\n    metrics_history = metrics_tracker.metrics_history\n    \n    return best_model, metrics_history, trainer.best_val_metric",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-20T20:35:30.394239Z",
     "iopub.execute_input": "2025-05-20T20:35:30.395055Z",
     "iopub.status.idle": "2025-05-20T20:35:30.403869Z",
     "shell.execute_reply.started": "2025-05-20T20:35:30.395011Z",
     "shell.execute_reply": "2025-05-20T20:35:30.402740Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## <font style=\"color:green\">5. Model [5 Points]</font>\n\n**Define your model in this section.**\n\n**You are allowed to use any pre-trained model.**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom efficientnet_pytorch import EfficientNet  # You might need to install this: pip install efficientnet-pytorch\n\nclass FoodClassifier(nn.Module):\n    \"\"\"\n    Model for Kenyan Food 13 classification using a pre-trained backbone.\n    \"\"\"\n    \n    def __init__(\n        self, \n        model_name: str = 'resnet50', \n        num_classes: int = 13, \n        pretrained: bool = True,\n        dropout_rate: float = 0.2\n    ):\n        \"\"\"\n        Initialize the model.\n        \n        Args:\n            model_name: Name of the pre-trained model to use as backbone\n                        (resnet18, resnet50, efficientnet_b0, etc.)\n            num_classes: Number of classes\n            pretrained: Whether to use pre-trained weights\n            dropout_rate: Dropout rate for the final layer\n        \"\"\"\n        super(FoodClassifier, self).__init__()\n        self.model_name = model_name.lower()\n        \n        # Initialize the backbone model\n        if 'resnet' in self.model_name:\n            if self.model_name == 'resnet18':\n                self.backbone = models.resnet18(pretrained=pretrained)\n                num_features = self.backbone.fc.in_features\n            elif self.model_name == 'resnet34':\n                self.backbone = models.resnet34(pretrained=pretrained)\n                num_features = self.backbone.fc.in_features\n            elif self.model_name == 'resnet50':\n                self.backbone = models.resnet50(pretrained=pretrained)\n                num_features = self.backbone.fc.in_features\n            elif self.model_name == 'resnet101':\n                self.backbone = models.resnet101(pretrained=pretrained)\n                num_features = self.backbone.fc.in_features\n            else:\n                raise ValueError(f\"Unsupported ResNet model: {model_name}\")\n            \n            # Replace the final fully connected layer\n            self.backbone.fc = nn.Identity()\n            \n            # Create a new classifier head\n            self.classifier = nn.Sequential(\n                nn.Dropout(dropout_rate),\n                nn.Linear(num_features, num_classes)\n            )\n            \n        elif 'efficientnet' in self.model_name:\n            # Format should be 'efficientnet_b0', 'efficientnet_b1', etc.\n            version = self.model_name.split('_')[1]\n            self.backbone = EfficientNet.from_pretrained(\n                f'efficientnet-{version}', \n                num_classes=num_classes,\n                include_top=False\n            )\n            num_features = self.backbone._fc.in_features\n            \n            # Replace the final classifier\n            self.backbone._fc = nn.Identity()\n            \n            # Create a new classifier head\n            self.classifier = nn.Sequential(\n                nn.Dropout(dropout_rate),\n                nn.Linear(num_features, num_classes)\n            )\n            \n        elif 'mobilenet' in self.model_name:\n            self.backbone = models.mobilenet_v2(pretrained=pretrained)\n            num_features = self.backbone.classifier[1].in_features\n            \n            # Replace the classifier\n            self.backbone.classifier = nn.Identity()\n            \n            # Create a new classifier head\n            self.classifier = nn.Sequential(\n                nn.Dropout(dropout_rate),\n                nn.Linear(num_features, num_classes)\n            )\n            \n        elif 'densenet' in self.model_name:\n            if self.model_name == 'densenet121':\n                self.backbone = models.densenet121(pretrained=pretrained)\n            elif self.model_name == 'densenet169':\n                self.backbone = models.densenet169(pretrained=pretrained)\n            elif self.model_name == 'densenet201':\n                self.backbone = models.densenet201(pretrained=pretrained)\n            else:\n                raise ValueError(f\"Unsupported DenseNet model: {model_name}\")\n                \n            num_features = self.backbone.classifier.in_features\n            \n            # Replace the classifier\n            self.backbone.classifier = nn.Identity()\n            \n            # Create a new classifier head\n            self.classifier = nn.Sequential(\n                nn.Dropout(dropout_rate),\n                nn.Linear(num_features, num_classes)\n            )\n            \n        else:\n            raise ValueError(f\"Unsupported model: {model_name}\")\n        \n    def forward(self, x):\n        \"\"\"Forward pass through the model.\"\"\"\n        features = self.backbone(x)\n        output = self.classifier(features)\n        return output",
   "metadata": {
    "trusted": true
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_model(config):\n    \"\"\"\n    Create model instance based on configuration.\n    \n    Args:\n        config: Configuration object with model parameters\n        \n    Returns:\n        Initialized model\n    \"\"\"\n    model = FoodClassifier(\n        model_name=config.model_name,\n        num_classes=config.num_classes,\n        pretrained=config.pretrained,\n        dropout_rate=config.dropout_rate\n    )\n    \n    return model",
   "metadata": {
    "trusted": true
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class FocalLoss(nn.Module):\n    \"\"\"\n    Focal Loss for dealing with class imbalance.\n    \n    Reference:\n    Lin, T. Y., Goyal, P., Girshick, R., He, K., & Dollár, P. (2017).\n    Focal loss for dense object detection.\n    \"\"\"\n    \n    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n        \"\"\"\n        Initialize Focal Loss.\n        \n        Args:\n            alpha: Weighting factor for the rare class\n            gamma: Focusing parameter\n            reduction: 'mean', 'sum', or 'none'\n        \"\"\"\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        self.cross_entropy = nn.CrossEntropyLoss(reduction='none')\n        \n    def forward(self, inputs, targets):\n        \"\"\"\n        Forward pass.\n        \n        Args:\n            inputs: Model predictions (logits), shape [B, C]\n            targets: Ground truth labels, shape [B]\n            \n        Returns:\n            Loss value\n        \"\"\"\n        # Standard cross entropy\n        ce_loss = self.cross_entropy(inputs, targets)\n        \n        # Get probabilities\n        probs = torch.exp(-ce_loss)\n        \n        # Apply focal weighting\n        focal_loss = self.alpha * (1 - probs)**self.gamma * ce_loss\n        \n        # Apply reduction\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:  # 'none'\n            return focal_loss",
   "metadata": {
    "trusted": true
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## <font style=\"color:green\">6. Utils [5 Points]</font>\n\n**Define those methods or classes, which have  not been covered in the above sections.**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport os\nimport pandas as pd\nfrom typing import Optional, List, Tuple, Dict, Any\nfrom torchvision.utils import make_grid\nimport torch.nn.functional as F\n\ndef set_seed(seed: int = 42):\n    \"\"\"\n    Set random seed for reproducibility.\n    \n    Args:\n        seed: Random seed\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef visualize_batch(batch, class_names=None, max_images=16, normalize=True):\n    \"\"\"\n    Visualize a batch of images.\n    \n    Args:\n        batch: Batch of images, shape [B, C, H, W]\n        class_names: List of class names\n        max_images: Maximum number of images to display\n        normalize: Whether to normalize the images\n        \n    Returns:\n        Matplotlib figure object\n    \"\"\"\n    images, labels = batch\n    \n    # Select a subset of images\n    batch_size = min(images.size(0), max_images)\n    images = images[:batch_size]\n    labels = labels[:batch_size]\n    \n    # Create grid of images\n    img_grid = make_grid(images, nrow=4, normalize=normalize)\n    \n    # Convert to numpy for display\n    img_grid = img_grid.cpu().numpy().transpose((1, 2, 0))\n    \n    # Create figure\n    fig, ax = plt.subplots(figsize=(12, 12))\n    ax.imshow(img_grid)\n    \n    # Add labels if class names are provided\n    if class_names is not None:\n        title = [class_names[label] for label in labels.cpu().numpy()]\n        ax.set_title('\\n'.join([', '.join(title[i:i+4]) for i in range(0, len(title), 4)]))\n    \n    ax.axis('off')\n    \n    return fig\n\ndef get_class_names(data_loader):\n    \"\"\"\n    Get class names from the dataset.\n    \n    Args:\n        data_loader: DataLoader with dataset\n        \n    Returns:\n        List of class names\n    \"\"\"\n    if hasattr(data_loader.dataset, 'idx_to_class'):\n        # Get indices and sort by value\n        indices = list(data_loader.dataset.idx_to_class.keys())\n        indices.sort()\n        \n        # Get class names in order\n        class_names = [data_loader.dataset.idx_to_class[i] for i in indices]\n        return class_names\n    else:\n        return None\n\ndef visualize_model_predictions(model, data_loader, class_names, device, num_images=16):\n    \"\"\"\n    Visualize model predictions on a batch of images.\n    \n    Args:\n        model: Trained model\n        data_loader: DataLoader with validation or test data\n        class_names: List of class names\n        device: Device to run inference on\n        num_images: Number of images to display\n        \n    Returns:\n        Matplotlib figure object\n    \"\"\"\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure(figsize=(16, 16))\n    \n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(data_loader):\n            if images_so_far >= num_images:\n                break\n                \n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            for j in range(inputs.size(0)):\n                if images_so_far >= num_images:\n                    break\n                    \n                images_so_far += 1\n                ax = plt.subplot(4, 4, images_so_far)\n                ax.axis('off')\n                \n                # Show image\n                img = inputs.cpu().data[j].numpy().transpose((1, 2, 0))\n                # Denormalize\n                mean = np.array([0.485, 0.456, 0.406])\n                std = np.array([0.229, 0.224, 0.225])\n                img = std * img + mean\n                img = np.clip(img, 0, 1)\n                \n                ax.imshow(img)\n                \n                # Show prediction\n                title = f'True: {class_names[labels[j]]}\\nPred: {class_names[preds[j]]}'\n                if preds[j] == labels[j]:\n                    title = f'{title}\\n(Correct)'\n                else:\n                    title = f'{title}\\n(Wrong)'\n                ax.set_title(title, color='green' if preds[j] == labels[j] else 'red')\n    \n    plt.tight_layout()\n    return fig\n\ndef create_submission_file(model, test_loader, device, output_path='submission.csv'):\n    \"\"\"\n    Create submission file with predicted classes.\n    \n    Args:\n        model: Trained model\n        test_loader: DataLoader with test data\n        device: Device to run inference on\n        output_path: Path to save the submission file\n        \n    Returns:\n        Path to the saved submission file\n    \"\"\"\n    model.eval()\n    all_ids = []\n    all_preds = []\n    \n    with torch.no_grad():\n        for inputs, ids in test_loader:\n            inputs = inputs.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            # Store predictions\n            all_ids.extend(ids)\n            all_preds.extend(preds.cpu().numpy())\n    \n    # Create DataFrame with predictions\n    submission_df = pd.DataFrame({\n        'ID': all_ids,\n        'CLASS': all_preds\n    })\n    \n    # Convert numerical labels to class names\n    if hasattr(test_loader.dataset, 'idx_to_class'):\n        submission_df['CLASS'] = submission_df['CLASS'].apply(\n            lambda x: test_loader.dataset.idx_to_class.get(x, x)\n        )\n    \n    # Save to CSV\n    submission_df.to_csv(output_path, index=False)\n    print(f\"Submission file saved to {output_path}\")\n    \n    return output_path",
   "metadata": {
    "trusted": true
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_optimizer(model, config):\n    \"\"\"\n    Create optimizer based on configuration.\n    \n    Args:\n        model: PyTorch model\n        config: Configuration object with optimizer parameters\n        \n    Returns:\n        PyTorch optimizer\n    \"\"\"\n    if config.optimizer.lower() == 'adam':\n        return optim.Adam(\n            model.parameters(),\n            lr=config.learning_rate,\n            weight_decay=config.weight_decay\n        )\n    elif config.optimizer.lower() == 'adamw':\n        return optim.AdamW(\n            model.parameters(),\n            lr=config.learning_rate,\n            weight_decay=config.weight_decay\n        )\n    elif config.optimizer.lower() == 'sgd':\n        return optim.SGD(\n            model.parameters(),\n            lr=config.learning_rate,\n            momentum=config.momentum,\n            weight_decay=config.weight_decay\n        )\n    else:\n        raise ValueError(f\"Unsupported optimizer: {config.optimizer}\")\n\ndef get_scheduler(optimizer, config):\n    \"\"\"\n    Create learning rate scheduler based on configuration.\n    \n    Args:\n        optimizer: PyTorch optimizer\n        config: Configuration object with scheduler parameters\n        \n    Returns:\n        PyTorch learning rate scheduler\n    \"\"\"\n    if config.lr_scheduler.lower() == 'plateau':\n        return ReduceLROnPlateau(\n            optimizer, mode='max', factor=config.lr_gamma,\n            patience=3, verbose=True\n        )\n    elif config.lr_scheduler.lower() == 'cosine':\n        return CosineAnnealingLR(\n            optimizer, T_max=config.num_epochs, eta_min=config.lr_min\n        )\n    elif config.lr_scheduler.lower() == 'step':\n        return StepLR(\n            optimizer, step_size=config.lr_step_size, gamma=config.lr_gamma\n        )\n    else:\n        return None\n\ndef get_loss_function(config):\n    \"\"\"\n    Create loss function based on configuration.\n    \n    Args:\n        config: Configuration object with loss parameters\n        \n    Returns:\n        PyTorch loss function\n    \"\"\"\n    if config.loss_fn.lower() == 'cross_entropy':\n        return nn.CrossEntropyLoss()\n    elif config.loss_fn.lower() == 'focal':\n        return FocalLoss(alpha=config.focal_alpha, gamma=config.focal_gamma)\n    else:\n        raise ValueError(f\"Unsupported loss function: {config.loss_fn}\")\n\ndef get_transforms(config):\n    \"\"\"\n    Create data transforms based on configuration.\n    \n    Args:\n        config: Configuration object with transform parameters\n        \n    Returns:\n        Tuple of (train_transform, val_transform)\n    \"\"\"\n    train_transform = transforms.Compose([\n        transforms.Resize((config.resize_size, config.resize_size)),\n        transforms.RandomCrop(config.crop_size),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(config.rotation_degrees) if config.use_augmentation else transforms.Lambda(lambda x: x),\n        transforms.ColorJitter(\n            brightness=config.color_jitter_factor,\n            contrast=config.color_jitter_factor,\n            saturation=config.color_jitter_factor,\n            hue=config.color_jitter_factor\n        ) if config.use_augmentation else transforms.Lambda(lambda x: x),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=config.mean, std=config.std)\n    ])\n    \n    val_transform = transforms.Compose([\n        transforms.Resize((config.img_size, config.img_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=config.mean, std=config.std)\n    ])\n    \n    return train_transform, val_transform\n\ndef visualize_learning_rate(optimizer, scheduler, num_epochs):\n    \"\"\"\n    Visualize learning rate schedule.\n    \n    Args:\n        optimizer: PyTorch optimizer\n        scheduler: PyTorch learning rate scheduler\n        num_epochs: Number of epochs\n        \n    Returns:\n        Matplotlib figure object\n    \"\"\"\n    lr_history = []\n    \n    # Save initial learning rate\n    lr_history.append(optimizer.param_groups[0]['lr'])\n    \n    # Step scheduler for each epoch\n    for _ in range(num_epochs):\n        if isinstance(scheduler, ReduceLROnPlateau):\n            # For ReduceLROnPlateau, assume validation accuracy improves and then plateaus\n            if _ < 5:\n                scheduler.step(0.5 + _/10)  # Simulate improving metric\n            else:\n                scheduler.step(0.5 + 5/10)  # Simulate plateauing metric\n        else:\n            scheduler.step()\n        \n        # Save learning rate\n        lr_history.append(optimizer.param_groups[0]['lr'])\n    \n    # Create figure\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.plot(range(num_epochs + 1), lr_history)\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Learning Rate')\n    ax.set_title('Learning Rate Schedule')\n    ax.grid(True)\n    \n    return fig",
   "metadata": {
    "trusted": true
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def analyze_model(model, val_loader, device, class_names):\n    \"\"\"\n    Analyze model performance on validation set.\n    \n    Args:\n        model: Trained model\n        val_loader: Validation data loader\n        device: Device to run inference on\n        class_names: List of class names\n        \n    Returns:\n        Dictionary with analysis results\n    \"\"\"\n    # Get predictions\n    true_labels, pred_labels, outputs = get_predictions(model, val_loader, device)\n    \n    # Convert outputs to probabilities\n    probs = F.softmax(outputs, dim=1).numpy()\n    \n    # Calculate metrics\n    metrics = {\n        'accuracy': EvaluationMetrics.accuracy(true_labels, pred_labels),\n        'precision': EvaluationMetrics.precision(true_labels, pred_labels),\n        'recall': EvaluationMetrics.recall(true_labels, pred_labels),\n        'f1': EvaluationMetrics.f1(true_labels, pred_labels),\n        'per_class_accuracy': EvaluationMetrics.per_class_accuracy(true_labels, pred_labels, len(class_names))\n    }\n    \n    # Generate confusion matrix\n    cm = EvaluationMetrics.confusion_matrix(true_labels, pred_labels)\n    \n    # Identify most confused classes\n    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    np.fill_diagonal(cm_norm, 0)  # Set diagonal to 0 to ignore correct predictions\n    \n    most_confused = []\n    for i in range(len(class_names)):\n        # Find top confusions for this class\n        confused_indices = np.argsort(cm_norm[i])[::-1][:3]  # Top 3 confusions\n        confusions = [(class_names[i], class_names[j], cm[i, j], cm_norm[i, j]) \n                     for j in confused_indices if cm_norm[i, j] > 0]\n        most_confused.extend(confusions)\n    \n    # Sort by confusion percentage\n    most_confused.sort(key=lambda x: x[3], reverse=True)\n    \n    # Format as DataFrame\n    if most_confused:\n        confused_df = pd.DataFrame(\n            most_confused, \n            columns=['True Class', 'Predicted Class', 'Count', 'Percentage']\n        )\n    else:\n        confused_df = pd.DataFrame(\n            columns=['True Class', 'Predicted Class', 'Count', 'Percentage']\n        )\n    \n    # Find most confident correct and incorrect predictions\n    class_probs = np.max(probs, axis=1)\n    correct_mask = pred_labels == true_labels\n    \n    # Most confident correct predictions\n    confident_correct_idx = np.argsort(class_probs * correct_mask)[::-1][:5]\n    confident_correct = [{\n        'idx': idx,\n        'true_label': class_names[true_labels[idx]],\n        'pred_label': class_names[pred_labels[idx]],\n        'confidence': class_probs[idx]\n    } for idx in confident_correct_idx if correct_mask[idx]]\n    \n    # Most confident incorrect predictions\n    confident_incorrect_idx = np.argsort(class_probs * ~correct_mask)[::-1][:5]\n    confident_incorrect = [{\n        'idx': idx,\n        'true_label': class_names[true_labels[idx]],\n        'pred_label': class_names[pred_labels[idx]],\n        'confidence': class_probs[idx]\n    } for idx in confident_incorrect_idx if not correct_mask[idx]]\n    \n    # Return analysis results\n    return {\n        'metrics': metrics,\n        'confusion_matrix': cm,\n        'most_confused': confused_df,\n        'confident_correct': confident_correct,\n        'confident_incorrect': confident_incorrect\n    }\n\ndef plot_class_distribution(train_loader, val_loader, class_names):\n    \"\"\"\n    Plot class distribution in training and validation sets.\n    \n    Args:\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        class_names: List of class names\n        \n    Returns:\n        Matplotlib figure object\n    \"\"\"\n    # Get class counts\n    train_counts = np.bincount([target for _, target in train_loader.dataset])\n    val_counts = np.bincount([target for _, target in val_loader.dataset])\n    \n    # Create figure\n    fig, ax = plt.subplots(figsize=(12, 6))\n    x = np.arange(len(class_names))\n    width = 0.35\n    \n    # Plot bars\n    ax.bar(x - width/2, train_counts, width, label='Train')\n    ax.bar(x + width/2, val_counts, width, label='Validation')\n    \n    # Add labels and legend\n    ax.set_xlabel('Class')\n    ax.set_ylabel('Count')\n    ax.set_title('Class Distribution')\n    ax.set_xticks(x)\n    ax.set_xticklabels(class_names, rotation=45, ha='right')\n    ax.legend()\n    \n    plt.tight_layout()\n    return fig",
   "metadata": {
    "trusted": true
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## <font style=\"color:green\">7. Experiment [5 Points]</font>\n\n**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Set random seed for reproducibility\nset_seed(42)\n\n# Create configuration\nconfig = TrainingConfiguration()\n\n# Adjust configuration for this experiment\nconfig.data_dir = \"/kaggle/input/opencv-pytorch-project-2-classification-round-3\"  # Kaggle path\nconfig.model_name = \"resnet50\"  # Using ResNet50 pre-trained model\nconfig.num_epochs = 20\nconfig.batch_size = 32\nconfig.learning_rate = 3e-4\nconfig.optimizer = \"adamw\"\nconfig.lr_scheduler = \"cosine\"\nconfig.weight_decay = 1e-4\nconfig.dropout_rate = 0.3\nconfig.use_augmentation = True\nconfig.mixed_precision = True  # Use mixed precision for faster training\nconfig.early_stopping_patience = 5",
   "metadata": {
    "trusted": true
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load the data\nprint(\"Loading data...\")\ntrain_loader, val_loader, test_loader = get_data(\n    data_dir=config.data_dir,\n    batch_size=config.batch_size,\n    num_workers=config.num_workers\n)\n\n# Get class names from the training dataset\nclass_names = get_class_names(train_loader)\nprint(f\"Class names: {class_names}\")\n\n# Visualize class distribution\nprint(\"Visualizing class distribution...\")\nclass_dist_fig = plot_class_distribution(train_loader, val_loader, class_names)\nplt.show()\n\n# Create the model\nprint(f\"Creating {config.model_name} model...\")\nmodel = get_model(config)\n\n# Create optimizer and scheduler\noptimizer = get_optimizer(model, config)\nscheduler = get_scheduler(optimizer, config)\n\n# Create loss function\ncriterion = get_loss_function(config)\n\n# Create metrics tracker\nmetrics_tracker = MetricsTracker(num_classes=config.num_classes, class_names=class_names)\n\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    config=config,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    metrics_tracker=metrics_tracker,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler\n)\n\n# Train the model\nprint(f\"Training {config.model_name} for {config.num_epochs} epochs...\")\nbest_model = trainer.train(monitor_metric='accuracy')\n\n# Analyze model performance on validation set\nprint(\"Analyzing model performance...\")\nanalysis = analyze_model(best_model, val_loader, torch.device(config.device), class_names)\n\n# Print metrics\nprint(\"\\nValidation Metrics:\")\nfor metric_name, value in analysis['metrics'].items():\n    if isinstance(value, np.ndarray):\n        continue\n    print(f\"{metric_name}: {value:.4f}\")\n\n# Print per-class accuracy\nprint(\"\\nPer-class Accuracy:\")\nfor i, acc in enumerate(analysis['metrics']['per_class_accuracy']):\n    print(f\"{class_names[i]}: {acc:.4f}\")\n\n# Plot confusion matrix\nprint(\"\\nPlotting confusion matrix...\")\ncm_fig = EvaluationMetrics.plot_confusion_matrix(\n    analysis['confusion_matrix'], \n    class_names=class_names, \n    normalize=True\n)\nplt.show()\n\n# Print most confused classes\nprint(\"\\nMost Confused Classes:\")\nprint(analysis['most_confused'].head(10))\n\n# Create submission file\nprint(\"\\nCreating submission file...\")\nsubmission_path = create_submission_file(\n    best_model, \n    test_loader, \n    torch.device(config.device),\n    output_path='submission.csv'\n)\n\nprint(f\"Submission file saved to {submission_path}\")\nprint(\"Done!\")",
   "metadata": {
    "trusted": true
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Ensemble Model Training (for better results)\n\n# Define configurations for ensemble models\nensemble_configs = [\n    # ResNet50 with different learning rate and augmentations\n    {\n        'model_name': 'resnet50',\n        'learning_rate': 2e-4,\n        'optimizer': 'adamw',\n        'weight_decay': 1e-4,\n        'dropout_rate': 0.3,\n        'use_augmentation': True,\n        'rotation_degrees': 20,\n        'color_jitter_factor': 0.15\n    },\n    # ResNet101 with different learning rate and augmentations\n    {\n        'model_name': 'resnet101',\n        'learning_rate': 1e-4,\n        'optimizer': 'adamw',\n        'weight_decay': 2e-4,\n        'dropout_rate': 0.2,\n        'use_augmentation': True,\n        'rotation_degrees': 15,\n        'color_jitter_factor': 0.1\n    },\n    # EfficientNet-B0 with different learning rate and augmentations\n    {\n        'model_name': 'efficientnet_b0',\n        'learning_rate': 3e-4,\n        'optimizer': 'adam',\n        'weight_decay': 1e-5,\n        'dropout_rate': 0.25,\n        'use_augmentation': True,\n        'rotation_degrees': 10,\n        'color_jitter_factor': 0.08\n    }\n]\n\n# Function to train ensemble models\ndef train_ensemble_models(base_config, model_configs, train_loader, val_loader, class_names):\n    \"\"\"\n    Train multiple models with different configurations for ensemble.\n    \n    Args:\n        base_config: Base configuration\n        model_configs: List of model-specific configurations\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        class_names: List of class names\n        \n    Returns:\n        List of trained models\n    \"\"\"\n    models = []\n    \n    for i, model_config in enumerate(model_configs):\n        print(f\"\\n=== Training Ensemble Model {i+1}/{len(model_configs)} ===\")\n        \n        # Create a copy of the base configuration\n        config = copy.deepcopy(base_config)\n        \n        # Update with model-specific configuration\n        for key, value in model_config.items():\n            setattr(config, key, value)\n        \n        # Create the model\n        print(f\"Creating {config.model_name} model...\")\n        model = get_model(config)\n        \n        # Create optimizer and scheduler\n        optimizer = get_optimizer(model, config)\n        scheduler = get_scheduler(optimizer, config)\n        \n        # Create loss function\n        criterion = get_loss_function(config)\n        \n        # Create metrics tracker\n        metrics_tracker = MetricsTracker(num_classes=config.num_classes, class_names=class_names)\n        \n        # Create trainer\n        trainer = Trainer(\n            model=model,\n            config=config,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            metrics_tracker=metrics_tracker,\n            criterion=criterion,\n            optimizer=optimizer,\n            scheduler=scheduler\n        )\n        \n        # Train the model\n        print(f\"Training {config.model_name} for {config.num_epochs} epochs...\")\n        best_model = trainer.train(monitor_metric='accuracy')\n        \n        # Add to ensemble\n        models.append(best_model)\n        print(f\"Best validation accuracy: {trainer.best_val_metric:.4f}\")\n    \n    return models\n\n# Function to get ensemble predictions\ndef get_ensemble_predictions(models, dataloader, device):\n    \"\"\"\n    Get ensemble predictions by averaging outputs from multiple models.\n    \n    Args:\n        models: List of trained models\n        dataloader: DataLoader for the dataset\n        device: Device to run inference on\n        \n    Returns:\n        If test set (no labels): (ids, predicted_labels)\n        If validation set (with labels): (true_labels, predicted_labels)\n    \"\"\"\n    # Check if this is the test set (no labels) or validation set (with labels)\n    batch = next(iter(dataloader))\n    is_test = len(batch) == 2 and not isinstance(batch[1], torch.Tensor)\n    \n    # Collect predictions from all models\n    all_outputs = []\n    \n    for model in models:\n        model.eval()\n        model_outputs = []\n        all_ids = []\n        all_labels = []\n        \n        with torch.no_grad():\n            for data in dataloader:\n                if is_test:\n                    inputs, ids = data\n                    all_ids.extend(ids)\n                else:\n                    inputs, labels = data\n                    all_labels.extend(labels.cpu().numpy())\n                \n                inputs = inputs.to(device)\n                outputs = model(inputs)\n                model_outputs.append(outputs)\n        \n        # Concatenate all outputs\n        model_outputs = torch.cat(model_outputs, dim=0)\n        all_outputs.append(model_outputs)\n    \n    # Average outputs from all models\n    ensemble_outputs = torch.mean(torch.stack(all_outputs), dim=0)\n    \n    # Get predicted labels\n    _, predicted = torch.max(ensemble_outputs, 1)\n    predicted = predicted.cpu().numpy()\n    \n    if is_test:\n        return all_ids, predicted\n    else:\n        return np.array(all_labels), predicted\n\n# Function to create ensemble submission file\ndef create_ensemble_submission(models, test_loader, device, output_path='ensemble_submission.csv'):\n    \"\"\"\n    Create submission file with ensemble predictions.\n    \n    Args:\n        models: List of trained models\n        test_loader: DataLoader with test data\n        device: Device to run inference on\n        output_path: Path to save the submission file\n        \n    Returns:\n        Path to the saved submission file\n    \"\"\"\n    # Get ensemble predictions\n    ids, predictions = get_ensemble_predictions(models, test_loader, device)\n    \n    # Create DataFrame with predictions\n    submission_df = pd.DataFrame({\n        'ID': ids,\n        'CLASS': predictions\n    })\n    \n    # Convert numerical labels to class names\n    if hasattr(test_loader.dataset, 'idx_to_class'):\n        submission_df['CLASS'] = submission_df['CLASS'].apply(\n            lambda x: test_loader.dataset.idx_to_class.get(x, x)\n        )\n    \n    # Save to CSV\n    submission_df.to_csv(output_path, index=False)\n    print(f\"Ensemble submission file saved to {output_path}\")\n    \n    return output_path\n\n# Uncomment to train ensemble models (this will take a while)\n\"\"\"\n# Train ensemble models\nprint(\"Training ensemble models...\")\nensemble_models = train_ensemble_models(\n    config, ensemble_configs, train_loader, val_loader, class_names\n)\n\n# Evaluate ensemble on validation set\nprint(\"\\nEvaluating ensemble on validation set...\")\nensemble_val_labels, ensemble_val_preds = get_ensemble_predictions(\n    ensemble_models, val_loader, torch.device(config.device)\n)\n\n# Calculate ensemble metrics\nensemble_accuracy = EvaluationMetrics.accuracy(ensemble_val_labels, ensemble_val_preds)\nensemble_f1 = EvaluationMetrics.f1(ensemble_val_labels, ensemble_val_preds)\n\nprint(f\"Ensemble Validation Accuracy: {ensemble_accuracy:.4f}\")\nprint(f\"Ensemble Validation F1 Score: {ensemble_f1:.4f}\")\n\n# Create ensemble submission file\nprint(\"\\nCreating ensemble submission file...\")\nensemble_submission_path = create_ensemble_submission(\n    ensemble_models, \n    test_loader, \n    torch.device(config.device),\n    output_path='ensemble_submission.csv'\n)\n\nprint(f\"Ensemble submission file saved to {ensemble_submission_path}\")\n\"\"\"",
   "metadata": {
    "trusted": true
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## <font style=\"color:green\">8. TensorBoard Log Link [5 Points]</font>\n\n**Share your TensorBoard scalars logs link here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n\n\nNote: In light of the recent shutdown of tensorboard.dev, we have updated the submission requirements for your project. Instead of sharing a tensorboard.dev link, you are now required to upload your generated TensorBoard event files directly onto the lab. As an alternative, you may also include a screenshot of your TensorBoard output within your Jupyter notebook. This adjustment ensures that your data visualization and model training efforts are thoroughly documented and accessible for evaluation.\n\nYou are also welcome (and encouraged) to utilize alternative logging services like wandB or comet. In such instances, you can easily make your project logs publicly accessible and share the link with others.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# TensorBoard Integration\n\n# TensorBoard scalars log link\n# Note: Since tensorboard.dev is no longer available, we'll save logs locally\n# and upload screenshots or event files instead.\n\n# Load tensorboard extension\n%load_ext tensorboard\n\n# Launch tensorboard\n%tensorboard --logdir=./runs\n\n# Alternative logging services:\n# 1. Weights & Biases (wandb): https://wandb.ai/\n# 2. Comet.ml: https://www.comet.ml/\n\n# Example code for wandb integration:\n\"\"\"\nimport wandb\n\n# Initialize wandb project\nwandb.init(project=\"kenyan-food-classification\", name=\"resnet50-experiment\")\n\n# Log hyperparameters\nwandb.config.update({\n    \"model\": config.model_name,\n    \"batch_size\": config.batch_size,\n    \"learning_rate\": config.learning_rate,\n    \"optimizer\": config.optimizer,\n    \"num_epochs\": config.num_epochs\n})\n\n# During training, log metrics\ndef log_metrics_to_wandb(metrics, epoch, phase):\n    \"\"\"Log metrics to wandb\"\"\"\n    wandb.log({\n        f\"{phase}_loss\": metrics['loss'],\n        f\"{phase}_accuracy\": metrics['accuracy'],\n        f\"{phase}_f1_score\": metrics['f1_score'],\n        \"epoch\": epoch\n    })\n\n# After training, log model\nwandb.save(config.model_checkpoint_path())\n\n# Close wandb run\nwandb.finish()\n\"\"\"\n\n# You can also include a screenshot of your TensorBoard here:\n# ![TensorBoard Screenshot](tensorboard_screenshot.png)\n\n# In the final submission, provide your TensorBoard event files or a link to your\n# wandb/comet dashboard for visualization of training metrics.",
   "metadata": {
    "trusted": true
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n\n**Share your Kaggle profile link  with us here to score , points in  the competition.**\n\n**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n\n\n**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Kaggle Profile Link\n\n# Replace the following with your actual Kaggle profile link\nkaggle_profile_link = \"https://www.kaggle.com/yourusername\"  # Replace with your Kaggle profile link\n\n# Note: To gain full points (50 points) for this section, you need:\n# 1. A minimum accuracy of 75% on the test data\n# 2. If accuracy is less than 70%, you gain no points for this section\n\n# For submission:\n# 1. Submit 'submission.csv' (prediction for images in 'test.csv') in the 'Submit Predictions' tab in Kaggle\n# 2. The file format should have two columns: 'ID' and 'CLASS'\n# 3. The 'ID' column contains the image IDs from test.csv\n# 4. The 'CLASS' column contains the predicted class for each image\n\n# Submission Tips:\n# - The ResNet50 model provided in this notebook should achieve >75% accuracy if trained properly\n# - If you want to improve results further, try:\n#   1. Ensemble methods (multiple models with different architectures)\n#   2. Advanced data augmentation techniques\n#   3. Test-time augmentation (TTA)\n#   4. Hyperparameter tuning\n\n# Competition Strategy:\n# 1. First establish a solid baseline (ResNet50 with proper training)\n# 2. Then try to improve with ensemble models\n# 3. Finally, fine-tune hyperparameters for best performance\n\nprint(f\"My Kaggle profile link: {kaggle_profile_link}\")",
   "metadata": {
    "trusted": true
   },
   "outputs": []
  }
 ]
}